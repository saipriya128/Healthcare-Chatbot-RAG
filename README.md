# Healthcare-Chatbot-RAG
This is a healthcare chatbot - Medichat, built using RAG approach and used LLaMA 2, LLaMA 3 models.

# Medical Chatbot Project

## Introduction
This project is a medical chatbot that utilizes Large Language Models (LLMs) to provide accurate and fast medical responses. It aims to assist healthcare providers and patients by offering quick access to medical information, patient interaction, and support in diagnosing health conditions.

## Key Features
- **High Accuracy and Speed**: Utilizes LLaMA 2â€™s and LLaMA 3's advanced natural language understanding capabilities to deliver precise medical information swiftly.
- **User-Friendly Interaction**: Simplifies complex medical terminologies and interactions through an intuitive chat interface, making advanced medical knowledge accessible to users without specialized training.
- **Dual Model Support**: Integrates both a locally hosted model (7 billion parameters) for enhanced privacy and an API-accessible model (13 billion parameters, 70 billion parameters) for extended capabilities, hosted on Replicate.
- **Advanced Data Processing**: Employs robust document loaders and text splitters to efficiently process local PDF files, turning static documents into dynamic data sources.
- **Efficient Information Retrieval**: Implements FAISS for vector storage and rapid similarity search, crucial for retrieving and comparing large volumes of document-based information.
- **Contextual Conversational Chain**: Maintains dialogue context and coherence through an advanced conversational retrieval system, ensuring that the chatbot can follow the thread of user interactions.
- **Interactive Web Interface**: Powered by Gradio, this feature allows users to interact directly with the chatbot, adjust model parameters in real-time, and visualize responses.

## Installation

### System Requirements
- Python 3.8+
- Virtual Environment (recommended)
- Internet connection for API model usage

### Setup Instructions
1. **UNZIP the Repository and change directory to chatbot:**
    cd medical-chatbot
   ```
2. **Install Dependencies:**
   ```bash
   pip install -r requirements.txt
   ```
3. **Set up the necessary API tokens and configurations:**
   - For the Replicate API, set the REPLICATE_API_TOKEN environment variable with your API token.
   - For local model setup, ensure you have the required model files (e.g., llama-2-7b-chat.ggmlv3.q8_0.bin) in the appropriate directory.

4. **Prepare Your Data:**
   - Place necessary PDF files within the `./data` directory to be processed locally.

## Usage

### Starting the Interface

- Run the python chatbot code in LLAMA_Chatbot.ipynb notebook
- Navigate to `http://127.0.0.1:7861` in your web browser to interact with the chatbot.

### Operating the Chatbot
- Select the desired model type (7 Billion-Model or 13 Billion-Model or 70 Billion-model) and adjust the model parameters as needed.
- Type your medical question in the input textbox and press Enter or click the submit button.
- The chatbot will process your query, retrieve relevant information from the embedded documents, and generate a response.
- You can continue the conversation by asking follow-up questions, and the chatbot will maintain context throughout the interaction.

## Detailed Setup

### Local Model (7 Billion Parameters)

 - Initialize the LLaMA 7B model with specific configurations:
   -- max_new_tokens: Maximum length of the generated response.
   -- temperature: Controls the randomness in the response generation.
   -- context_length: Specifies how much context the model should consider.
 - Load local PDF documents using the PyPDFDirectoryLoader and split them into manageable chunks using RecursiveCharacterTextSplitter.
 - Set up the embedding model (e.g., sentence-transformers/all-mpnet-base-v2) and create a vector store using FAISS for efficient similarity search.
 - Initialize a conversational retrieval chain with the LLaMA model and the vector store for answering queries.


### API Model (13 Billion Parameters, 70 BIllion Parameters)


- Set up the Replicate API token as an environment variable.
- Define the model identifier for the LLaMA 13B model hosted on Replicate.
- Initialize the LLaMA 13B model with specific parameters:
- temperature: Controls the randomness in the response generation.
- top_p: Specifies the cumulative probability threshold for token selection.
- max_new_tokens: Maximum length of the generated response.
- Follow the same steps as the local model setup for document loading, text splitting, embedding, and vector store creation.
- Set up a conversational retrieval chain using the LLaMA 13B model and the vector store for answering queries.


## Response Comparison
To compare the responses generated by different models or configurations, you can use the compare_responses function provided in the code. This function takes two responses as input and calculates the cosine similarity between their BERT embeddings. A higher similarity score indicates more similarity between the responses.

## Gradio Interface

The project includes a Gradio-based web interface for easy interaction with the chatbot. The interface allows users to select the model type  and adjust various model parameters such as max new tokens, temperature, top p, and context length. Users can type their medical questions in the input textbox, and the chatbot will generate a response based on the selected model and parameters.


## Conclusion
This medical chatbot project demonstrates the power of Large Language Models like LLaMA 2 in providing accurate and fast medical responses. By leveraging advanced natural language processing techniques, efficient document retrieval, and user-friendly interfaces, it aims to assist healthcare providers and patients in accessing relevant medical information and support.


